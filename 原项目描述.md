好的，这是一段重新整理、连贯叙述的关于“WonkaVision”项目的描述：

---

**探索沉浸式网络：WonkaVision 与视觉错觉的复兴**

我们一直在思考一个核心问题：能否利用简单的视觉错觉，让网络体验更具沉浸感？这个探索将我们带回了技术演示史上一个令人难忘的时刻：2007年，约翰尼·李（Johnny Lee）巧妙地利用 Wii 遥控器和传感器条，实现了令人惊叹的 VR 显示效果。尽管过去了十几年，那份演示的震撼力依然新鲜。

约翰尼的技术广受赞誉，甚至登上了 TED 舞台，他本人后来也参与了 Kinect 和 Project Tango 等项目。然而，令人遗憾的是，他那巧妙的 VR 显示技术从未被整合到 Wii 游戏或任何我们所知的商业产品中。今年早些时候重温他的演示时，我们不禁追问：为何如此受欢迎的技术未能普及？人人都想尝试，却无人将其发扬光大。

这引发了我们新的好奇：能否仅凭普通的网络摄像头，在浏览器中复现这种效果？如果可行，这将极大地降低技术的门槛，在网络上开启一个全新的互动体验世界。想象一下，打开你喜爱的品牌网站，眼前出现一个微缩的虚拟店面，仿佛站在人行道上窥视实体店铺，浏览其热门产品——这个愿景让我们兴奋不已，并立即付诸行动。

我们的探索始于核心挑战：**使用网络摄像头进行精确的 3D 眼球追踪**。约翰尼依赖 Wii 遥控器的红外摄像头和传感器条上的 LED 来定位头部，而我们则需用普通 RGB 摄像头达到类似效果，即精确计算出“右眼在摄像头右侧 20 厘米，上方 10 厘米，前方 50 厘米”这样的世界空间坐标。研究后，我们发现了 Google 的 MediaPipe Iris 库，它声称能通过单 RGB 摄像头实时追踪虹膜标志点，并利用人眼虹膜平均直径（约 11.7 毫米）这一特性，结合几何原理计算出眼睛到摄像头的距离（误差 < 10%）。这简直是理想方案！可惜，MediaPipe Iris 当时并不支持 JavaScript，而我们坚持要在浏览器中实现。

峰回路转，我们发现一篇博客文章指出，Google 的另一工具 MediaPipe Face Mesh 也能间接实现虹膜深度测量。其原理是：Face Mesh 能检测虹膜边缘的左、右、上、下四个点。知道左右两点的实际物理距离（即虹膜直径，约 11.7 毫米）及其在图像中的像素距离，再结合网络摄像头的**内参**（有效焦距 fx, fy 和主点 cx, cy），就可以利用针孔相机模型方程求解出眼睛在空间中的深度（Z 坐标），进而推算出其 X 和 Y 坐标。关键步骤在于如何获得摄像头的内参。参照另一篇博客的指导，我们打印了棋盘格图案，用摄像头拍摄了约 50 张不同角度的照片，再用 OpenCV 脚本检测角点并计算出内参。通过眼镜上贴棍子沿尺子移动的“土法”验证，我们确认了计算深度的准确性，虽不如 MediaPipe Iris 理想，但已足够用于我们的目标。

解决了眼球追踪，下一步是**将屏幕打造成虚拟窗口**。我们希望屏幕内容能根据观看角度和眼睛到屏幕的距离动态变化。为此，我们必须在游戏引擎中精确重建现实世界。我们将虚拟世界的原点设定为网络摄像头的位置。屏幕本身在虚拟世界中被建模为一个粉色矩形，其物理尺寸（宽、高）以及摄像头到屏幕顶部的距离都被精确测量并设置。这种对现实尺寸的严格复现至关重要：只有确保虚拟世界中的屏幕位置、尺寸与物理屏幕完全一致，并且虚拟相机（代表用户眼睛）的位置（由 MediaPipe 计算得出）在虚拟世界中得到精确映射，才能实现逼真的视觉错觉。放置在“虚拟窗口”后的物体将显现深度感，而置于其前的物体则仿佛跃出屏幕。

另一个关键挑战是**透视投影的选择**。标准的轴上透视投影会产生对称的金字塔形视锥体，当用户移动头部时，会在电脑屏幕上看到“窗口”边缘之外的内容，破坏了“窗口”的错觉。我们转而采用**离轴透视投影**（off-axis perspective projection）。其视锥体是非对称的，底座恰好贴合虚拟窗口的矩形。这样，无论用户如何移动头部，在屏幕上看到的景象始终被限制在“窗口”边界之内，内容随视角自然变化，这正是我们追求的效果。

这里需要指出这项技术的一个重要**局限性**：为了获得最强的“物体跃出屏幕”的深度错觉，用户需要将虚拟相机（渲染视角）严格放置在*一只眼睛*（左眼或右眼）的位置，并*闭上另一只眼睛*。如果使用双眼之间的中点作为虚拟相机位置，或者用户双眼都睁开，虽然仍能观察到有趣的角度变化效果，但那种强烈的深度感和物体“跳出”屏幕的惊人错觉会大大减弱。这与 VR 头显的工作原理（为每只眼独立渲染）类似，而我们只有一块屏幕，因此只能服务于单眼视角。

克服重重技术障碍后，我们最终在浏览器中仅凭网络摄像头成功复现了约翰尼·李的技术，并称之为 **“WonkaVision”**（灵感源自《查理和巧克力工厂》中伸手进电视抓取巧克力棒的经典场景）。我们实现了三个核心演示：
1.  **约翰尼的目标演示复刻**：将 iPad 摄像头紧贴右眼，目标物仿佛真实漂浮在屏幕外，效果震撼。
2.  **深度与运动演示（僵尸场景）**：展示深度感和相机运动，僵尸试图挣脱屏幕的设定突显了技术的潜力。
3.  **虚拟店面演示（以 Teenage Engineering OP-1 为例）**：产品仿佛悬浮于屏幕之外，产生强烈的“伸手可触”的沉浸感。

回顾最初的问题——**为何约翰尼·李的技术未能普及？**——在亲身经历了整个实现过程后，我们有了清晰的答案：
1.  **硬件参数获取困难**：需要精确的网络摄像头内参（需繁琐的棋盘格标定）和屏幕的物理尺寸（现代浏览器无法直接查询）。
2.  **用户体验限制**：最佳效果要求用户闭上一只眼睛，这对主流应用来说是个障碍。
这些因素使得该技术难以无缝、大规模地部署。即使未来参数获取变得容易，用户是否愿意为沉浸感而闭上一只眼睛仍是未知数。

尽管“WonkaVision”可能难以成为普及的 XR 解决方案，但这个项目本身就是一个有力的证明：现代网络浏览器的能力已今非昔比。借助 WebAssembly，复杂的机器学习模型（如 MediaPipe）可以轻松加载运行，实现实时的眼动、面部、手部追踪。结合强大的渲染能力，以流畅帧率呈现精美场景已成为可能。虽然 Johnny Lee 的特定光学错觉之路充满荆棘，但浏览器作为平台所展现的潜力，让我们对网络交互的未来充满了兴奋和期待。

---